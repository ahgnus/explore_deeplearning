{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3271bd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e60e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1729b0b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 197463\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91274</th>\n",
       "      <td>Tom thought Mary was alone.</td>\n",
       "      <td>Tom pensait que Marie était seule.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155185</th>\n",
       "      <td>I was going to ask you the same thing.</td>\n",
       "      <td>J'allais vous demander la même chose.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97745</th>\n",
       "      <td>Tell me about your children.</td>\n",
       "      <td>Parle-moi de tes enfants.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103938</th>\n",
       "      <td>I'm just as tired as you are.</td>\n",
       "      <td>Je suis aussi fatigué que toi.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89535</th>\n",
       "      <td>Of course Tom will be late.</td>\n",
       "      <td>Evidemment Tom sera en retard.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           eng  \\\n",
       "91274              Tom thought Mary was alone.   \n",
       "155185  I was going to ask you the same thing.   \n",
       "97745             Tell me about your children.   \n",
       "103938           I'm just as tired as you are.   \n",
       "89535              Of course Tom will be late.   \n",
       "\n",
       "                                          fra  \\\n",
       "91274      Tom pensait que Marie était seule.   \n",
       "155185  J'allais vous demander la même chose.   \n",
       "97745               Parle-moi de tes enfants.   \n",
       "103938         Je suis aussi fatigué que toi.   \n",
       "89535          Evidemment Tom sera en retard.   \n",
       "\n",
       "                                                       cc  \n",
       "91274   CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "155185  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "97745   CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "103938  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "89535   CC-BY 2.0 (France) Attribution: tatoeba.org #7...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#샘플 데이터 불러옴\n",
    "import os\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "309de852",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20962</th>\n",
       "      <td>Tom can't see us.</td>\n",
       "      <td>Tom ne peut pas nous voir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4440</th>\n",
       "      <td>Are you deaf?</td>\n",
       "      <td>Êtes-vous sourde ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12942</th>\n",
       "      <td>You're kidding!</td>\n",
       "      <td>Sans blague !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32528</th>\n",
       "      <td>Tom drank too much.</td>\n",
       "      <td>Tom a trop bu.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27063</th>\n",
       "      <td>We can solve this.</td>\n",
       "      <td>Nous pouvons résoudre ceci.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       eng                          fra\n",
       "20962    Tom can't see us.   Tom ne peut pas nous voir.\n",
       "4440         Are you deaf?           Êtes-vous sourde ?\n",
       "12942      You're kidding!                Sans blague !\n",
       "32528  Tom drank too much.               Tom a trop bu.\n",
       "27063   We can solve this.  Nous pouvons résoudre ceci."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3번째 열은 불필요하므로 제거하고, 훈련 데이터는 33000개의 샘플로 줄임\n",
    "lines = lines[['eng', 'fra']][:33000] # 33000개 샘플 사용 3000개는 추후 테스트 샘플로 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "739b9c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16531</th>\n",
       "      <td>Turn the fan on.</td>\n",
       "      <td>\\t Allume le ventilateur. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>I'm fast.</td>\n",
       "      <td>\\t Je suis rapide. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25910</th>\n",
       "      <td>That makes me cry.</td>\n",
       "      <td>\\t Ça me fait pleurer. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16904</th>\n",
       "      <td>What's its name?</td>\n",
       "      <td>\\t Quel est son nom ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19845</th>\n",
       "      <td>It's very simple.</td>\n",
       "      <td>\\t C'est très simple. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      eng                           fra\n",
       "16531    Turn the fan on.  \\t Allume le ventilateur. \\n\n",
       "658             I'm fast.         \\t Je suis rapide. \\n\n",
       "25910  That makes me cry.     \\t Ça me fait pleurer. \\n\n",
       "16904    What's its name?      \\t Quel est son nom ? \\n\n",
       "19845   It's very simple.      \\t C'est très simple. \\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '\\t'\n",
    "eos_token = '\\n'\n",
    "lines.fra = lines.fra.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f781700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 데이터 프레임 칼럼을 리스트화\n",
    "eng_list = lines['eng'].to_list()\n",
    "fra_list = lines['fra'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7d592a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.', 'Go.', 'Go.', 'Go.', 'Hi.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a8c86f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 구두점을 단어와 분리\n",
    "import re, string\n",
    "\n",
    "def punc(s): return re.sub(f\"([{string.punctuation}])\", r' \\1 ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aee2402",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_el = [punc(i) for i in eng_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bae29a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go . ', 'Go . ', 'Go . ', 'Go . ', 'Hi . ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_el[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35e193ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fl = [punc(i) for i in fra_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93295e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\t Va  !  \\n',\n",
       " '\\t Marche .  \\n',\n",
       " '\\t En route  !  \\n',\n",
       " '\\t Bouge  !  \\n',\n",
       " '\\t Salut  !  \\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_fl[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b28870b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[29, 1], [29, 1], [29, 1]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 소문자로 바꾸고 띄어쓰기 단위로 토큰화 \n",
    "eng_tokenizer = Tokenizer(char_level=False, filters='', lower =True)   # 단어 단위로 Tokenizer를 생성합니다. 소문자로 바꾸고 필터(구두점제거)\n",
    "eng_tokenizer.fit_on_texts(p_el)               # 33000개의 행을 가진 p_el의 각 요소들에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(p_el)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "327ae899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 76, 10, 2], [1, 345, 3, 2], [1, 27, 486, 10, 2]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=False, filters='', lower =True)   # 단어 단위로 Tokenizer를 생성합니다. 소문자로 바꾸고 필터(구두점제거)\n",
    "fra_tokenizer.fit_on_texts(p_fl)               # 33000개의 행을 가진 p_fl의 각 요소들에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(p_fl)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2005d719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 4713\n",
      "프랑스어 단어장의 크기 : 9467\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "605f1318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 10\n",
      "프랑스어 시퀀스의 최대 길이 20\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d897723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 4713\n",
      "프랑스어 단어장의 크기 : 9467\n",
      "영어 시퀀스의 최대 길이 10\n",
      "프랑스어 시퀀스의 최대 길이 20\n"
     ]
    }
   ],
   "source": [
    "#전체적인 통계정보를 한꺼번에 출력\n",
    "\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d75b970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8e07bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 76, 10], [1, 345, 3], [1, 27, 486, 10]]\n",
      "[[76, 10, 2], [345, 3, 2], [27, 486, 10, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a01a56aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 10)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 20)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 20)\n"
     ]
    }
   ],
   "source": [
    "#데이터의 크기 확인\n",
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c87aac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29  1  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cb05511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 벡터화 원핫 인코딩\n",
    "# encoder_input = to_categorical(encoder_input)\n",
    "# decoder_input = to_categorical(decoder_input)\n",
    "# decoder_target = to_categorical(decoder_target)\n",
    "# print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "# print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "# print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31f192ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (33000, 10)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (33000, 20)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (33000, 20)\n"
     ]
    }
   ],
   "source": [
    "# validation 생성 3000건 나머지 학습데이터로\n",
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c5b1c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#프랑수아 숄레의 케라스의 seq2seq 구현 가이드 게시물인 A ten-minute introduction to sequence-to-sequence learning를 참고\n",
    "#도구 임포트\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64daaec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#임베딩 층 구현\n",
    "from tensorflow.keras.layers import Input, Embedding, Masking\n",
    "\n",
    "# 인코더에서 사용할 임베딩 층 사용 예시(영어) 임베딩 벡터의 차원(word_vec_dim)을 8으로 설정 하이퍼 파라미터\n",
    "word_vec_dim = 512\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(eng_vocab_size, word_vec_dim )(encoder_inputs)\n",
    "# hidden size가 16인 인코더의 LSTM 셀 생성\n",
    "encoder_lstm = LSTM(units = 256, return_state=True)\n",
    "# hidden state와 cell state를 다음 time step으로 전달하기 위해서 별도 저장.\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ebe47728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#디코더 설계\n",
    "# 입력 텐서 생성. word_vec_dim 16 동일\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb =  Embedding(fra_vocab_size, word_vec_dim )(decoder_inputs)\n",
    "# hidden size가 16인 디코더의 LSTM 셀 생성\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "# decoder_outputs는 모든 time step의 hidden state\n",
    "decoder_outputs, _, _= decoder_lstm(dec_emb, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c404c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#디코더의 출력층 설계\n",
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64987674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 512)    2413056     input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 512)    4847104     input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 256), (None, 787456      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, None, 256),  787456      embedding_5[0][0]                \n",
      "                                                                 lstm_4[0][1]                     \n",
      "                                                                 lstm_4[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 9467)   2433019     lstm_5[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 11,268,091\n",
      "Trainable params: 11,268,091\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#매 time step마다의 다중 클래스 분류 문제이므로 프랑스어 단어장으로부터 한 가지 문자만 선택하도록 합니다. Dense의 인자로 프랑스어 단어장의 크기를 기재하고, 활성화 함수로 소프트맥스 함수를 사용. 최종적으로 인코더와 디코더를 연결해서 하나의 모델로 만들어줍니다. Model의 Input과 Output의 정의를 유심히 살펴 주세요.\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2cfdba6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "118/118 [==============================] - 9s 53ms/step - loss: 1.8971 - val_loss: 1.5633\n",
      "Epoch 2/100\n",
      "118/118 [==============================] - 6s 48ms/step - loss: 1.1930 - val_loss: 1.3590\n",
      "Epoch 3/100\n",
      "118/118 [==============================] - 6s 49ms/step - loss: 1.0162 - val_loss: 1.2391\n",
      "Epoch 4/100\n",
      "118/118 [==============================] - 6s 49ms/step - loss: 0.8990 - val_loss: 1.1385\n",
      "Epoch 5/100\n",
      "118/118 [==============================] - 6s 49ms/step - loss: 0.8162 - val_loss: 1.0697\n",
      "Epoch 6/100\n",
      "118/118 [==============================] - 6s 50ms/step - loss: 0.7517 - val_loss: 1.0291\n",
      "Epoch 7/100\n",
      "118/118 [==============================] - 6s 50ms/step - loss: 0.6972 - val_loss: 0.9925\n",
      "Epoch 8/100\n",
      "118/118 [==============================] - 6s 50ms/step - loss: 0.6501 - val_loss: 0.9570\n",
      "Epoch 9/100\n",
      "118/118 [==============================] - 6s 50ms/step - loss: 0.6083 - val_loss: 0.9441\n",
      "Epoch 10/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.5712 - val_loss: 0.9252\n",
      "Epoch 11/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.5377 - val_loss: 0.9069\n",
      "Epoch 12/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.5066 - val_loss: 0.8864\n",
      "Epoch 13/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.4785 - val_loss: 0.8736\n",
      "Epoch 14/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.4519 - val_loss: 0.8791\n",
      "Epoch 15/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.4271 - val_loss: 0.8630\n",
      "Epoch 16/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.4031 - val_loss: 0.8682\n",
      "Epoch 17/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.3809 - val_loss: 0.8537\n",
      "Epoch 18/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.3608 - val_loss: 0.8590\n",
      "Epoch 19/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.3417 - val_loss: 0.8490\n",
      "Epoch 20/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.3229 - val_loss: 0.8522\n",
      "Epoch 21/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.3055 - val_loss: 0.8552\n",
      "Epoch 22/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.2894 - val_loss: 0.8424\n",
      "Epoch 23/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.2751 - val_loss: 0.8440\n",
      "Epoch 24/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.2617 - val_loss: 0.8536\n",
      "Epoch 25/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.2489 - val_loss: 0.8423\n",
      "Epoch 26/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.2372 - val_loss: 0.8473\n",
      "Epoch 27/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.2267 - val_loss: 0.8505\n",
      "Epoch 28/100\n",
      "118/118 [==============================] - 6s 51ms/step - loss: 0.2165 - val_loss: 0.8530\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "earlystopping = EarlyStopping(monitor= 'val_loss', patience= 3)\n",
    "\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "                    validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "                    batch_size=256, epochs=100, callbacks = earlystopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2eedff3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, None, 512)         2413056   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                [(None, 256), (None, 256) 787456    \n",
      "=================================================================\n",
      "Total params: 3,200,512\n",
      "Trainable params: 3,200,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#테스트 단계의 디코더의 동작 순서\n",
    "#인코더에 입력 문장을 넣어 마지막 time step의 hidden, cell state를 얻는다.\n",
    "# 토큰인 '\\t'를 디코더에 입력한다.\n",
    "# 이전 time step의 출력층의 예측 결과를 현재 time step의 입력으로 한다.\n",
    "# 3을 반복하다가 토큰인 '\\n'가 예측되면 이를 중단한다.\n",
    "\n",
    "#모델 테스트하기\n",
    "#인코더 정의\n",
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1107ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(dec_emb, initial_state = decoder_states_inputs)\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aaeb956a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 512)    4847104     input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, None, 256),  787456      embedding_5[0][0]                \n",
      "                                                                 input_11[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 9467)   2433019     lstm_5[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 8,067,579\n",
      "Trainable params: 8,067,579\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#출력층 재설계\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9b9dbb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dfa3ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # 에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1,1)) \n",
    "    target_seq[0, 0] = fra2idx['\\t']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # 에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장     \n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e1b1824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Go.\n",
      "정답 문장:  Bouge ! \n",
      "번역기가 번역한 문장:  va ! \n",
      "-----------------------------------\n",
      "입력 문장: Hello!\n",
      "정답 문장:  Bonjour ! \n",
      "번역기가 번역한 문장:  bonjour ! \n",
      "-----------------------------------\n",
      "입력 문장: Got it?\n",
      "정답 문장:  T'as capté ? \n",
      "번역기가 번역한 문장:  est - ce que ça a ét\n",
      "-----------------------------------\n",
      "입력 문장: Hang on.\n",
      "정답 문장:  Tiens bon ! \n",
      "번역기가 번역한 문장:  attendez . \n",
      "-----------------------------------\n",
      "입력 문장: Here's $5.\n",
      "정답 문장:  Voilà cinq dollars. \n",
      "번역기가 번역한 문장:  voici là . \n",
      "-----------------------------------\n",
      "입력 문장: No way!\n",
      "정답 문장:  Hors de question ! \n",
      "번역기가 번역한 문장:  sans question ! \n",
      "-----------------------------------\n",
      "입력 문장: I'm hit!\n",
      "정답 문장:  Je suis touché ! \n",
      "번역기가 번역한 문장:  je suis touché ! \n",
      "-----------------------------------\n",
      "입력 문장: I stood.\n",
      "정답 문장:  Je me suis tenu debout. \n",
      "번역기가 번역한 문장:  je me suis mis à pleure\n",
      "-----------------------------------\n",
      "입력 문장: Get out!\n",
      "정답 문장:  Dégage ! \n",
      "번역기가 번역한 문장:  sortez ! \n",
      "-----------------------------------\n",
      "입력 문장: Thanks.\n",
      "정답 문장:  Je te remercie. \n",
      "번역기가 번역한 문장:  merci ! \n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001, 134, 345, 332, 234, 141]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60fe157",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "1. 입력 문장: Go.\n",
    "정답 문장:  Bouge ! \n",
    "번역기가 번역한 문장:  va !  \n",
    "##### 정답 문장과 다름 프랑스어 의미도 다름, 그러나 스페인어로 간다는 의미임 va ! 학습시에 스페인어가 더 많이 쓰였나?\n",
    "-----------------------------------\n",
    "2. 입력 문장: Hello!\n",
    "정답 문장:  Bonjour ! \n",
    "번역기가 번역한 문장:  bonjour !  \n",
    "##### 정답! \n",
    "-----------------------------------\n",
    "3. 입력 문장: Got it?\n",
    "정답 문장:  T'as capté ? \n",
    "번역기가 번역한 문장:  est - ce que ça a ét  \n",
    "#####  의미 같음\n",
    "-----------------------------------\n",
    "4. 입력 문장: Hang on.\n",
    "정답 문장:  Tiens bon ! \n",
    "번역기가 번역한 문장:  attendez .   \n",
    "#####  의미 같음\n",
    "-----------------------------------\n",
    "5. 입력 문장: Here's $5.\n",
    "정답 문장:  Voilà cinq dollars. \n",
    "번역기가 번역한 문장:  voici là . \n",
    "##### 정답 문장과 다름, 의미는 비슷하나 5달러가 빠져있음\n",
    "-----------------------------------\n",
    "6. 입력 문장: No way!\n",
    "정답 문장:  Hors de question ! \n",
    "번역기가 번역한 문장:  sans question !  \n",
    "##### 의미 같음\n",
    "-----------------------------------\n",
    "7. 입력 문장: I'm hit!\n",
    "정답 문장:  Je suis touché ! \n",
    "번역기가 번역한 문장:  je suis touché ! \n",
    "##### 정답!\n",
    "-----------------------------------\n",
    "8. 입력 문장: I stood.\n",
    "정답 문장:  Je me suis tenu debout. \n",
    "번역기가 번역한 문장:  je me suis mis à pleure  \n",
    "##### 정답 문장과 다름, 의미도 달라보임\n",
    "----------------------------------- \n",
    "9. 입력 문장: Get out!\n",
    "정답 문장:  Dégage ! \n",
    "번역기가 번역한 문장:  sortez !       \n",
    "##### 의미 같음\n",
    "-----------------------------------\n",
    "10. 입력 문장: Thanks.\n",
    "정답 문장:  Je te remercie. \n",
    "번역기가 번역한 문장:  merci !  \n",
    "##### 의미 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bd79f2",
   "metadata": {},
   "source": [
    "- #### 영어에서 바로 프랑스어 1:1 대응이 아닌 \n",
    "- #### 영어 -> X -> 프랑스어로 인코딩, 디코딩 그리고 출력을 설계함\n",
    "- #### 모델이 학습 후 마치 영어의 의미를 이해하여 프랑스어로 바꿔주는 듯한 느낌을 받았다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a559149c",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "- 지난 ex 6에서는 직접 구두점들을 설정하고 다루는 코드를 작성하여 사용하였으나,\n",
    "- 이번에는 string 내장 함수를 사용하여 구두점 작업을 하였다.\n",
    "- sentence to sentence 의 번역기 코드를 작성해 볼 수 있었다.\n",
    "- 직접 임베딩 레이어를 추가하여 단어의 임베딩 깊이를 조절해 볼 수 있었다.\n",
    "- 영어를 인코딩 하여 어떤 x로 바꾸고 그 x를 디코딩하여 프랑스어로 바꾸는 알고리즘으로 러닝 모델을 작성해 보았다. \n",
    "- Total params를 최대한으로 줄여서 모델 학습을 시켜보려고 하였으나 클라우드 서버가 계속 다운이 되어 진행시키지 못했다.\n",
    "- 코드 자체의 문제인지 많은사람이 한꺼번에 코드를 돌려서 서버 리소스의 문제인지 확인을 하지 못하였다.\n",
    "-  코드 자체에서 임베딩을 할때 원핫 인코딩으로 30000개의 단어로 30000X30000의 인코딩이 생겨 발생한 것으로 확인 \n",
    "-  원핫 인코딩을 하지않고, encoding layer 설계할때에 인스턴스로 인코딩을 하는 레이어를 추가하여 학습시킴 \n",
    "- 지난번 데이터톤 때에도 느꼈지만 장비(GPU TPU등 연산장치 또는 머신러닝을 구현하는데 필요한 부수 장비)의 성능 및 발전도 머신러닝을 연구하고 발전시키는데 아주 중요한 부분임을 새삼 깨달았다\n",
    "- colab또는 좋은 머신러닝구현 환경을 사용하도록 준비를 해야겠다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
